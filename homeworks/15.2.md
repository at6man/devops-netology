## Задание 1. Яндекс.Облако

[Код для создания ресурсов через терраформ](https://github.com/at6man/devops-netology/tree/main/homeworks/terraform_15.2)   

Код рабочий, все проверено. Тут поясню некоторые нюансы и проблемы, которые были, и задам вопросы.

### Проблема 1

Я хотел вывести IP всех созданных ВМ и IP балансировщика сразу в терраформе (в outputs). Т.е. чтобы все было через terraform, а не использовать другие средства для узнавания этих IP.

В outputs.tf показано, как я это сделал, в итоге. Но это я сделал через несколько отладок ошибок terraform plan/apply.

И все равно, я не смог добраться чисто до отдельных IP (например, IP балансировщика), а вывод был вот такой:

    external_ip_address_lb = tolist([
      toset([
        {
          "address" = "51.250.72.143"
          "ip_version" = "ipv4"
        },
      ]),
    ])
    external_ip_address_vm = tolist([
      "51.250.77.73",
      "51.250.65.67",
      "51.250.94.58",
    ])
    internal_ip_address_vm = tolist([
      "192.168.10.6",
      "192.168.10.4",
      "192.168.10.21",
    ])

Т.е. как будто до поля address балансировщика не пробраться напрямую вообще (из-за вложенных структур list, set и т.п.).

И по документации https://registry.terraform.io/providers/yandex-cloud/yandex/latest/docs/resources/lb_network_load_balancer данная структура вообще никак не следует.

Есть ли какой-то способ увидеть точную структуру данных провайдера заранее? Или вообще нет смысла тратить время на эти outputs?

### Нюанс 2

Для проверки балансировщика добавлял в html-странички внутренние IP-адреса отдельных ВМ. Так нагляднее видно, с какой ВМ отдается контент по IP балансировщика.

И там видно, что сетевой балансировщик работает по своим каким-то правилам, т.е. он может много раз подряд отдавать с 1 сервера, и только потом на другой переключиться. И на этот алгоритм мы не можем повлиять, видимо (судя по документации Яндекса).

А чтобы задавать алгоритм балансировщика, это надо использовать application load balancer.

### Проблема 3

При выполнении `terraform destroy` все зависает на удалении группы ВМ (висит бесконечно долго).

Как оказалось, причина в том, что сначала почему-то удаляются ресурсы прав сервисных аккаунтов, например, yandex_resourcemanager_folder_iam_binding.editor. И видимо, без этих прав уже далее не может ничего выполняться с группой ВМ, т.к. группа ВМ управляется сервисным аккаунтом.

Видимо, дело в том, что в ресурсе yandex_compute_instance_group есть ссылка только на yandex_iam_service_account, а ссылки на yandex_resourcemanager_folder_iam_binding там нет. Поэтому, терраформ не считает группу ВМ зависящей от этого биндинга, и удаляет биндинг независимо от группы ВМ (т.е. раньше).

Позже придумал (вспомнил), что наверное блок `depends_on` помог бы установить зависимость и нужный порядок создания/удаления:

    resource "yandex_compute_instance_group" "ig-1" {
      ...
      depends_on {
        yandex_resourcemanager_folder_iam_binding.editor
      }
    }
