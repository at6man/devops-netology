## Задание 1: установить в кластер CNI плагин Calico

### установка производится через ansible/kubespray

Делаем аналогично предыдущему ДЗ:

    alex@AlexAsusLinux:~/netology/kubernetes-for-beginners/99-misc$ yc compute instance list
    +----------------------+-------+---------------+---------+---------------+-------------+
    |          ID          | NAME  |    ZONE ID    | STATUS  |  EXTERNAL IP  | INTERNAL IP |
    +----------------------+-------+---------------+---------+---------------+-------------+
    | ef30c54o8vufsjt6vp5h | node1 | ru-central1-c | RUNNING | 51.250.32.161 | 10.130.0.25 |
    | ef3a2374ndgctbh1vtde | cp1   | ru-central1-c | RUNNING | 51.250.33.174 | 10.130.0.23 |
    +----------------------+-------+---------------+---------+---------------+-------------+

    alex@AlexAsusLinux:~/netology/kubespray$ vi inventory/mycluster/hosts.yaml

    all:
      hosts:
        cp1:
          ansible_host: 51.250.33.174
          ansible_user: yc-user
        node1:
          ansible_host: 51.250.32.161
          ansible_user: yc-user
      children:
        kube_control_plane:
          hosts:
            cp1:
        kube_node:
          hosts:
            node1:
        etcd:
          hosts:
            cp1:
        k8s_cluster:
          children:
            kube_control_plane:
            kube_node:
        calico_rr:
          hosts: {}

    alex@AlexAsusLinux:~/netology/kubespray$ vi inventory/mycluster/group_vars/all/all.yml
    ...
    loadbalancer_apiserver:
       address: 51.250.33.174
       port: 6443
    ...

Настройка `kube_network_plugin: calico` стояла по умолчанию в inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml

    alex@AlexAsusLinux:~/netology/kubespray$ ansible-playbook -i inventory/mycluster/hosts.yaml cluster.yml -b -v
    ...
    PLAY RECAP ********************************************************************************************************
    cp1                        : ok=744  changed=147  unreachable=0    failed=0    skipped=1304 rescued=0    ignored=6   
    localhost                  : ok=4    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
    node1                      : ok=481  changed=90   unreachable=0    failed=0    skipped=764  rescued=0    ignored=2
    ...

Копируем /root/.kube/config c cp1 на localhost в ~/.kube/config, прописываем внешний IP кластера: `server: https://51.250.33.174:6443`

    alex@AlexAsusLinux:~/netology/kubespray$ kubectl get nodes
    NAME    STATUS   ROLES                  AGE     VERSION
    cp1     Ready    control-plane,master   11m     v1.23.6
    node1   Ready    <none>                 9m41s   v1.23.6

Убедимся, что CNI-плагин запущен:

    alex@AlexAsusLinux:~/netology/kubespray$ kubectl get pods -A
    NAMESPACE     NAME                                       READY   STATUS    RESTARTS        AGE
    kube-system   calico-kube-controllers-58dfb4874f-fhf7q   1/1     Running   0               9m41s
    kube-system   calico-node-8nzgd                          1/1     Running   0               10m
    kube-system   calico-node-c59m4                          1/1     Running   0               10m
    ...

### настроить политику доступа к hello-world извне

    alex@AlexAsusLinux:~/netology/kubespray$ kubectl create deployment hello-node --image=k8s.gcr.io/echoserver:1.4
    deployment.apps/hello-node created

    alex@AlexAsusLinux:~/netology/kubespray$ kubectl create deployment network-multitool --image=praqma/network-multitool:alpine-extra
    deployment.apps/network-multitool created

    alex@AlexAsusLinux:~/netology/kubespray$ kubectl get pods
    NAME                                 READY   STATUS    RESTARTS   AGE
    hello-node-6b89d599b9-2xnv6          1/1     Running   0          113s
    network-multitool-84f977d8c7-tlxxf   1/1     Running   0          26s

    alex@AlexAsusLinux:~/netology/kubespray$ kubectl expose deployment hello-node --port=8080
    service/hello-node exposed

    alex@AlexAsusLinux:~/netology/kubespray$ kubectl get services
    NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
    hello-node   ClusterIP   10.233.30.145   <none>        8080/TCP   32s
    kubernetes   ClusterIP   10.233.0.1      <none>        443/TCP    25m

Видим, что доступ к сервису hello-node с других подов не ограничен:

    alex@AlexAsusLinux:~/netology/kubespray$ kubectl exec network-multitool-84f977d8c7-tlxxf -- curl -s -m 1 http://hello-node:8080/
    CLIENT VALUES:
    client_address=10.233.90.3
    command=GET
    real path=/
    query=nil
    request_version=1.1
    request_uri=http://hello-node:8080/

    SERVER VALUES:
    server_version=nginx: 1.10.0 - lua: 10001

    HEADERS RECEIVED:
    accept=*/*
    host=hello-node:8080
    user-agent=curl/7.79.1
    BODY:
    -no body in request-

Применим такую политику:

    apiVersion: networking.k8s.io/v1
    kind: NetworkPolicy
    metadata:
      name: default-deny-ingress
      namespace: default
    spec:
      podSelector: {}
      policyTypes:
        - Ingress

    alex@AlexAsusLinux:~/netology/kubernetes-for-beginners/16-networking/20-network-policy/templates$ kubectl apply -f network-policy/00-default.yaml 
    networkpolicy.networking.k8s.io/default-deny-ingress created

Теперь достучаться из другого пода не можем:

    alex@AlexAsusLinux:~/netology/kubespray$ kubectl exec network-multitool-84f977d8c7-tlxxf -- curl -s -m 1 http://hello-node:8080/
    command terminated with exit code 28

Посмотрим на описание пода:

    alex@AlexAsusLinux:~/netology/kubespray$ kubectl edit pod network-multitool-84f977d8c7-tlxxf

    apiVersion: v1
    kind: Pod
    metadata:
      annotations:
        cni.projectcalico.org/containerID: 42442e105fc3c8ab3e3d6ff4bdb1d98fcc347a4c2de60f07d411b35ee1e3d114
        cni.projectcalico.org/podIP: 10.233.90.3/32
        cni.projectcalico.org/podIPs: 10.233.90.3/32
      creationTimestamp: "2022-05-06T02:20:35Z"
      generateName: network-multitool-84f977d8c7-
      labels:
        app: network-multitool
        pod-template-hash: 84f977d8c7
    ...

Видим метку app=network-multitool. Используем ее для создания политики, чтобы разрешить доступ к hello-node только из подов с меткой app=network-multitool.  
  
Создаем файл с такой политикой:

    apiVersion: networking.k8s.io/v1
    kind: NetworkPolicy
    metadata:
      name: hello-node
      namespace: default
    spec:
      podSelector:
        matchLabels:
          app: hello-node
      policyTypes:
        - Ingress
      ingress:
        - from:
          - podSelector:
              matchLabels:
                app: network-multitool
          ports:
            - protocol: TCP
              port: 8080
          
Применяем этот файл:

    alex@AlexAsusLinux:~/netology$ kubectl apply -f 12-5-network-policy.yaml 
    networkpolicy.networking.k8s.io/hello-node created

Проверяем доступность сервиса, и видим, что снова все в порядке:

    alex@AlexAsusLinux:~/netology$ kubectl exec network-multitool-84f977d8c7-tlxxf -- curl -s -m 1 http://hello-node:8080/
    CLIENT VALUES:
    client_address=10.233.90.3
    command=GET
    real path=/
    query=nil
    request_version=1.1
    request_uri=http://hello-node:8080/

    SERVER VALUES:
    server_version=nginx: 1.10.0 - lua: 10001

    HEADERS RECEIVED:
    accept=*/*
    host=hello-node:8080
    user-agent=curl/7.79.1
    BODY:
    -no body in request-

Создадим еще 1 деплоймент:

    alex@AlexAskubectl create deployment network-multitool-2 --image=praqma/network-multitool:alpine-extraitool:alpine-extra
    deployment.apps/network-multitool-2 created

Из нового пода достучаться не можем, т.к. у него другой label:

    alex@AlexAsusLinux:~/netology$ kubectl exec network-multitool-2-cfdf779d4-nx2n8 -- curl -s -m 1 http://hello-node:8080/
    command terminated with exit code 28

## Задание 2: изучить, что запущено по умолчанию

### установить утилиту calicoctl

    root@AlexAsusLinux:/home/alex/netology# cd /usr/local/bin/
    root@AlexAsusLinux:/usr/local/bin# curl -L https://github.com/projectcalico/calico/releases/download/v3.22.2/calicoctl-linux-amd64 -o calicoctl
    root@AlexAsusLinux:/usr/local/bin# chmod +x ./calicoctl

### получить список нод, ipPool и profile в консоли

    alex@AlexAsusLinux:~/netology$ calicoctl get node
    NAME    
    cp1     
    node1   

    alex@AlexAsusLinux:~/netology$ calicoctl get ipPool
    NAME           CIDR             SELECTOR   
    default-pool   10.233.64.0/18   all()      

    alex@AlexAsusLinux:~/netology$ calicoctl get profile
    NAME                                                 
    projectcalico-default-allow                          
    kns.default                                          
    kns.kube-node-lease                                  
    kns.kube-public                                      
    kns.kube-system                                      
    ksa.default.default                                  
    ksa.kube-node-lease.default                          
    ksa.kube-public.default                              
    ksa.kube-system.attachdetach-controller              
    ksa.kube-system.bootstrap-signer                     
    ksa.kube-system.calico-kube-controllers              
    ksa.kube-system.calico-node                          
    ksa.kube-system.certificate-controller               
    ksa.kube-system.clusterrole-aggregation-controller   
    ksa.kube-system.coredns                              
    ksa.kube-system.cronjob-controller                   
    ksa.kube-system.daemon-set-controller                
    ksa.kube-system.default                              
    ksa.kube-system.deployment-controller                
    ksa.kube-system.disruption-controller                
    ksa.kube-system.dns-autoscaler                       
    ksa.kube-system.endpoint-controller                  
    ksa.kube-system.endpointslice-controller             
    ksa.kube-system.endpointslicemirroring-controller    
    ksa.kube-system.ephemeral-volume-controller          
    ksa.kube-system.expand-controller                    
    ksa.kube-system.generic-garbage-collector            
    ksa.kube-system.horizontal-pod-autoscaler            
    ksa.kube-system.job-controller                       
    ksa.kube-system.kube-proxy                           
    ksa.kube-system.namespace-controller                 
    ksa.kube-system.node-controller                      
    ksa.kube-system.nodelocaldns                         
    ksa.kube-system.persistent-volume-binder             
    ksa.kube-system.pod-garbage-collector                
    ksa.kube-system.pv-protection-controller             
    ksa.kube-system.pvc-protection-controller            
    ksa.kube-system.replicaset-controller                
    ksa.kube-system.replication-controller               
    ksa.kube-system.resourcequota-controller             
    ksa.kube-system.root-ca-cert-publisher               
    ksa.kube-system.service-account-controller           
    ksa.kube-system.service-controller                   
    ksa.kube-system.statefulset-controller               
    ksa.kube-system.token-cleaner                        
    ksa.kube-system.ttl-after-finished-controller        
    ksa.kube-system.ttl-controller
