    alex@AlexAsusLinux:~/netology$ git clone https://github.com/kubernetes-sigs/kubespray

    alex@AlexAsusLinux:~/netology$ cd kubespray/

    alex@AlexAsusLinux:~/netology/kubespray$ sudo pip3 install -r requirements.txt
    ...
    Successfully installed ansible-core-2.12.5 cffi-1.15.0 cryptography-2.8 jmespath-0.9.5 netaddr-0.7.19 pbr-5.4.4 pycparser-2.21 ruamel.yaml-0.16.10 ruamel.yaml.clib-0.2.6

    alex@AlexAsusLinux:~/netology/kubespray$ cp -rfp inventory/sample inventory/mycluster

    alex@AlexAsusLinux:~/netology/kubespray$ yc compute instance list
    +----------------------+-------+---------------+---------+---------------+-------------+
    |          ID          | NAME  |    ZONE ID    | STATUS  |  EXTERNAL IP  | INTERNAL IP |
    +----------------------+-------+---------------+---------+---------------+-------------+
    | ef389mn2818v2pnl3gi1 | node1 | ru-central1-c | RUNNING | 51.250.44.174 | 10.130.0.5  |
    | ef398pesft3lbq1glg76 | node4 | ru-central1-c | RUNNING | 51.250.38.208 | 10.130.0.31 |
    | ef3dd0t2rjro078pnf07 | node3 | ru-central1-c | RUNNING | 51.250.32.209 | 10.130.0.14 |
    | ef3m1r77m2k1afu2lgts | node2 | ru-central1-c | RUNNING | 51.250.36.198 | 10.130.0.20 |
    | ef3m7b319ivaelq0jt6g | cp1   | ru-central1-c | RUNNING | 51.250.37.87  | 10.130.0.17 |
    +----------------------+-------+---------------+---------+---------------+-------------+

    alex@AlexAsusLinux:~/netology/kubespray$ vi inventory/mycluster/hosts.yaml
    
    all:
      hosts:
        cp1:
          ansible_host: 51.250.37.87
          ansible_user: yc-user
        node1:
          ansible_host: 51.250.44.174
          ansible_user: yc-user
        node2:
          ansible_host: 51.250.36.198
          ansible_user: yc-user
        node3:
          ansible_host: 51.250.32.209
          ansible_user: yc-user
        node4:
          ansible_host: 51.250.38.208
          ansible_user: yc-user
      children:
        kube_control_plane:
          hosts:
            cp1:
        kube_node:
          hosts:
            node1:
            node2:
            node3:
            node4:
        etcd:
          hosts:
            cp1:
        k8s_cluster:
          children:
            kube_control_plane:
            kube_node:
        calico_rr:
          hosts: {}

Настройка `container_manager: containerd` стояла по умолчанию в `inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml`

    alex@AlexAsusLinux:~/netology/kubespray$ vi inventory/mycluster/group_vars/all/all.yml
    ...
    loadbalancer_apiserver:
       address: 51.250.37.87
       port: 6443
    ...

    alex@AlexAsusLinux:~/netology/kubespray$ ansible-playbook -i inventory/mycluster/hosts.yaml cluster.yml -b -v
    ...
    PLAY RECAP ********************************************************************************************************
    cp1                        : ok=744  changed=147  unreachable=0    failed=0    skipped=1304 rescued=0    ignored=6   
    localhost                  : ok=4    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
    node1                      : ok=481  changed=90   unreachable=0    failed=0    skipped=764  rescued=0    ignored=2   
    node2                      : ok=481  changed=90   unreachable=0    failed=0    skipped=764  rescued=0    ignored=2   
    node3                      : ok=481  changed=90   unreachable=0    failed=0    skipped=764  rescued=0    ignored=2   
    node4                      : ok=481  changed=90   unreachable=0    failed=0    skipped=764  rescued=0    ignored=2
    ...

    alex@AlexAsusLinux:~/netology/kubespray$ ssh yc-user@51.250.37.87
    yc-user@cp1:/etc/kubernetes$ sudo su
    
    root@cp1:/etc/kubernetes# kubectl get nodes
    NAME    STATUS   ROLES                  AGE   VERSION
    cp1     Ready    control-plane,master   30m   v1.23.6
    node1   Ready    <none>                 28m   v1.23.6
    node2   Ready    <none>                 28m   v1.23.6
    node3   Ready    <none>                 28m   v1.23.6
    node4   Ready    <none>                 28m   v1.23.6

Копируем /root/.kube/config c cp1 на localhost в ~/.kube/config, прописываем внешний IP кластера: `server: https://51.250.37.87:6443`  
Получаем список нод на локальной машине:

    alex@AlexAsusLinux:~/.kube$ kubectl get nodes
    NAME    STATUS   ROLES                  AGE   VERSION
    cp1     Ready    control-plane,master   39m   v1.23.6
    node1   Ready    <none>                 37m   v1.23.6
    node2   Ready    <none>                 37m   v1.23.6
    node3   Ready    <none>                 37m   v1.23.6
    node4   Ready    <none>                 37m   v1.23.6

    alex@AlexAsusLinux:~/.kube$ kubectl create deploy nginx --image=nginx:latest --replicas=2
    deployment.apps/nginx created
    
    alex@AlexAsusLinux:~/.kube$ kubectl get pods -o wide
    NAME                     READY   STATUS    RESTARTS   AGE   IP            NODE    NOMINATED NODE   READINESS GATES
    nginx-7c658794b9-gxhhd   1/1     Running   0          17s   10.233.92.1   node3   <none>           <none>
    nginx-7c658794b9-hr52g   1/1     Running   0          17s   10.233.96.1   node2   <none>           <none>
