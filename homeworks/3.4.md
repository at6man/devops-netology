1.
        # Чтобы выбрать, какой релиз скачать отсюда: https://github.com/prometheus/node_exporter/releases
        
        vagrant@vagrant:~$ uname -a
        Linux vagrant 5.4.0-73-generic #82-Ubuntu SMP Wed Apr 14 17:39:42 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux
        vagrant@vagrant:~$ cat /proc/cpuinfo  | grep 'name'| uniq
        model name      : AMD A10-4600M APU with Radeon(tm) HD Graphics
        
        vagrant@vagrant:~$ mkdir progs
        vagrant@vagrant:~$ cd progs
        vagrant@vagrant:~/progs$ wget https://github.com/prometheus/node_exporter/releases/download/v1.1.2/node_exporter-1.1.2.linux-amd64.tar.gz
        vagrant@vagrant:~/progs$ tar -xzf node_exporter-1.1.2.linux-amd64.tar.gz
        vagrant@vagrant:~/progs$ cd node_exporter-1.1.2.linux-amd64
        vagrant@vagrant:~/progs/node_exporter-1.1.2.linux-amd64$ sudo cp node_exporter /usr/local/bin/
        vagrant@vagrant:~$ cd ~
        vagrant@vagrant:~$ sudo useradd -rs /bin/false node_exporter
        vagrant@vagrant:~$ sudo vi /etc/systemd/system/node_exporter.service
        
           [Unit]
           Description=Node Exporter
           After=network.target
           
           [Service]
           User=node_exporter
           Group=node_exporter
           Type=simple
           ExecStart=/usr/local/bin/node_exporter
           
           [Install]
           WantedBy=multi-user.target
        
        vagrant@vagrant:~$ sudo systemctl daemon-reload
        vagrant@vagrant:~$ sudo systemctl start node_exporter
        
        vagrant@vagrant:~$ sudo systemctl status node_exporter
        vagrant@vagrant:~$ sudo systemctl enable node_exporter
        Created symlink /etc/systemd/system/multi-user.target.wants/node_exporter.service → /etc/systemd/system/node_exporter.service.
        
        # Меняем 2 строки в секции [Service]
        vagrant@vagrant:~$ sudo vim /etc/systemd/system/node_exporter.service
   
           [Service]
           ...
           EnvironmentFile=/etc/node_exporter
           ExecStart=/usr/local/bin/node_exporter $COMMAND_LINE_OPTS
        
        vagrant@vagrant:~$ sudo vim /etc/node_exporter
        
           COMMAND_LINE_OPTS=--collector.disable-defaults --collector.filesystem
        
        vagrant@vagrant:~$ sudo systemctl stop node_exporter
        vagrant@vagrant:~$ sudo systemctl daemon-reload
        vagrant@vagrant:~$ sudo systemctl start node_exporter
        vagrant@vagrant:~$ sudo systemctl status node_exporter
        ● node_exporter.service - Node Exporter
             Loaded: loaded (/etc/systemd/system/node_exporter.service; enabled; vendor preset: enabled)
             Active: active (running) since Sat 2021-07-03 12:11:59 UTC; 2s ago
           Main PID: 2181 (node_exporter)
              Tasks: 5 (limit: 2280)
             Memory: 2.1M
             CGroup: /system.slice/node_exporter.service
                     └─2181 /usr/local/bin/node_exporter --collector.disable-defaults --collector.filesystem

2. Не совсем понятно, что имеется в виду под опциями в данном задании. Тут https://github.com/prometheus/node_exporter сказано: "See the --help output for more options", и в хелпе мы видим описание параметров (опций) командной строки. Если речь о них, то можно было бы передавать такие опции:

   --collector.disable-defaults --collector.cpu --collector.meminfo --collector.filesystem --collector.netstat и т.п. (но я их все не тестировал).  
   Если же речь именно о метриках, то основные для базового мониторинга можно было бы выделить такие:
	- node_cpu_seconds_total
	- node_memory_MemAvailable_bytes
	- node_filesystem_avail_bytes
	- node_network_receive_bytes_total
	- node_network_transmit_bytes_total
   

3. Конфиг `/etc/netdata/netdata.conf` почти пустой, вверху написано, как получить полный конфиг:
   
         sudo wget -O /etc/netdata/netdata.conf http://localhost:19999/netdata.conf
         sudo vim /etc/netdata/netdata.conf
         # прописал bind to = 0.0.0.0 в секции [web]
   
   Зашел в браузере на http://localhost:19999/ , увидел много красивых графиков, обновляющихся в реальном времени


4. Нашел вот такие записи, указывающие, что ОС осознает наличие виртуализации:
   - CPU MTRRs all blank - virtualized system.
   - Booting paravirtualized kernel on KVM
   - Performance Events: PMU not available due to virtualization, using software events only.
   - systemd[1]: Detected virtualization oracle.
   

5. 
         vagrant@vagrant:~$ sysctl fs.nr_open
         fs.nr_open = 1048576

   Не совсем понятно, где посмотреть официальный man по этим параметрам ядра?

         man proc
   
	> /proc/sys/fs/nr_open (since Linux 2.6.25)<br>
              This  file  imposes  ceiling  on  the value to which the RLIMIT_NOFILE resource limit can be raised (see getrlimit(2)).  This ceiling is enforced for both unprivileged and
              privileged process.  The default value in this file is 1048576.  (Before Linux 2.6.25, the ceiling for RLIMIT_NOFILE was hard-coded to the same value.)

         man getrlimit

   > The getrlimit() and setrlimit() system calls get and set resource limits.  Each resource has an associated soft and hard limit<br>
RLIMIT_NOFILE<br>
              This  specifies  a value one greater than the maximum file descriptor number that can be opened by this process.

   Т.е. получается, этот параметр (fs.nr_open) задает максимальное кол-во файловых дескрипторов, которое может открыть отдельный процесс (то кол-во, до которого можно поднять этот лимит другими способами).

         vagrant@vagrant:~$ ulimit --help
            -n        the maximum number of open file descriptors
         vagrant@vagrant:~$ ulimit -n
         1024
         vagrant@vagrant:~$ ulimit -Hn
         1048576
         vagrant@vagrant:~$ ulimit -n 1048576
         vagrant@vagrant:~$ ulimit -n
         1048576

   Т.е. можно поднять мягкий лимит до жесткого в текущей сессии через ulimit.  
   А вот при попытке менять жесткий лимит через ulimit, заметил, что он легко уменьшается, а вот обратно увеличить его не получается почему-то :)
   
6.
         vagrant@vagrant:~$ sudo -i
         root@vagrant:~# screen
         root@vagrant:~# unshare -f --pid --mount-proc sleep 1h
   
   `ctrl + A`  
   `D`
   
         root@vagrant:~# pstree -p
                    ├─screen(3000)───bash(3001)───unshare(3008)───sleep(3009)
         root@vagrant:~# nsenter --target 3009 --pid --mount
         root@vagrant:/# ps aux
         USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
         root           1  0.0  0.0   8076   592 pts/1    S+   13:00   0:00 sleep 1h
         root           2  0.0  0.4   9836  4064 pts/0    S    13:02   0:00 -bash
         root          11  0.0  0.3  11492  3416 pts/0    R+   13:02   0:00 ps aux

7. `:(){ :|:& };:` - это форк-бомба на языке Bash. Двоеточие - это название функции в данном случае. В теле функции (в фигурных скобках) происходит вызов такой же фукнции 2 раза через пайплайн и в фоновом режиме. В конце команды ф-ция запускается 1 раз (чтобы запустить механизм этой "бомбы" :)  
   Найти описание этого кода не сложно. Что интересного в `dmesg`:
   
         vagrant@vagrant:~$ dmesg -T
         [Sun Jul  4 13:25:42 2021] cgroup: fork rejected by pids controller in /user.slice/user-1000.slice/session-15.scope
         [Sun Jul  4 13:28:36 2021] INFO: task jbd2/dm-0-8:297 blocked for more than 120 seconds.
         ...
         [Sun Jul  4 13:28:36 2021] INFO: task bash:3851 blocked for more than 120 seconds.
         ...
         [Sun Jul  4 13:28:36 2021] INFO: task bash:4610 blocked for more than 120 seconds.

   Вопрос в задании: **какой механизм помог автоматической стабилизации. Как настроен этот механизм по-умолчанию, и как изменить число процессов, которое можно создать в сессии??**  
   Ответить не так просто, много гуглил по этим фразам из `dmesg`, но какой-то четкой картины этого "механизма" не сложилось. Видимо, этот механизм - это "pids controller", который является частью механизма cgroup в ядре Linux; он контролирует лимит на кол-во созданных процессов в сессии пользователя. Как изменить лимит числа процессов в сессии - не совсем понял. Нашел, как можно изменить кое-что, вот примерно такие команды я делал:
   
         vagrant@vagrant:~$ cat /sys/fs/cgroup/pids/user.slice/user-1000.slice/pids.max
         2359
         vagrant@vagrant:~$ sudo systemctl edit user.slice
         
            [Slice]
            TasksMax=5555
         
         # тут видим внизу, что новая настройка добавилась:
         vagrant@vagrant:~$ sudo systemctl cat user.slice
         
            # /lib/systemd/system/user.slice
            #  SPDX-License-Identifier: LGPL-2.1+
            #
            #  This file is part of systemd.
            #
            #  systemd is free software; you can redistribute it and/or modify it
            #  under the terms of the GNU Lesser General Public License as published by
            #  the Free Software Foundation; either version 2.1 of the License, or
            #  (at your option) any later version.
            
            [Unit]
            Description=User and Session Slice
            Documentation=man:systemd.special(7)
            Before=slices.target
            
            # /etc/systemd/system/user.slice.d/override.conf
            [Slice]
            TasksMax=5555
         
         # Это не изменилось:
         vagrant@vagrant:~$ cat /sys/fs/cgroup/pids/user.slice/user-1000.slice/pids.max
         2359
         
         # Тут изменилось:
         vagrant@vagrant:~$ systemctl status user.slice
         ● user.slice - User and Session Slice
              Loaded: loaded (/lib/systemd/system/user.slice; static; vendor preset: enabled)
             Drop-In: /etc/systemd/system/user.slice.d
                      └─override.conf
              Active: active since Sat 2021-07-03 21:57:25 UTC; 16h ago
                Docs: man:systemd.special(7)
               Tasks: 7 (limit: 5555)
              Memory: 23.4M
         
         # Также заметил, что тут изменилось:
         vagrant@vagrant:~$ cat /sys/fs/cgroup/pids/user.slice/pids.max
         5555

   Как изменить значение именно в `/sys/fs/cgroup/pids/user.slice/user-1000.slice/pids.max` - так и не понял. Находил информацию, что нужно создать файл `/etc/systemd/logind.conf.d/override.conf` и в нем записать:

         [Login]
         UserTasksMax=<значение>
   
   Но у меня это не сработало... (после перезагрузки тоже).  
   Есть еще такая настройка:
   
         vagrant@vagrant:~$ cat /sys/fs/cgroup/pids/user.slice/user-1000.slice/session-4.scope/pids.max
         max

   Есть подозрение, что значение "max" означает безлимит, но он перебивается значениями из родительских уровней, который у меня 2359 и 5555. Какое реальное значение будет использовано, 2359 или 5555, - не понятно.  
   Где найти понятный официальный man или др. четкую информацию по поводу этих "механизмов"? В `man cgroups` есть упоминание `pids.max` вскользь, но это не проясняет ситуацию.  
   Также изучил `man logind.conf` и `man systemd.resource-control`  
   Вот тут https://serverfault.com/questions/885841/how-do-i-set-cgroup-limits-for-systemd-user-slices пишут, что в Ubuntu 16.04 лимит UserTasksMax был описан в `man logind.conf`, но в нашей версии Ubuntu его там нет, и он не срабатывает, соответственно. Куда он переехал - не могу найти :)